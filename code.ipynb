{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TZXR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def read_feature_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            content = content.strip('[]')\n",
    "            features = [float(x.strip()) for x in content.split(',')]\n",
    "            return np.array(features)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_layoutxlm_features(product_id, province, base_path):\n",
    "    product_path = os.path.join(base_path, province, str(product_id), '')\n",
    "    if not os.path.exists(product_path):\n",
    "        return None\n",
    "    cls_path = os.path.join(product_path, '')\n",
    "    img_path = os.path.join(product_path, '')\n",
    "    cls_features = read_feature_file(cls_path)\n",
    "    img_features = read_feature_file(img_path)\n",
    "    if cls_features is None or img_features is None:\n",
    "        return None\n",
    "    return np.concatenate([cls_features, img_features])\n",
    "\n",
    "def get_visual_features(product_id, province, base_path):\n",
    "    visual_path = os.path.join(base_path, province, str(product_id), '', '')\n",
    "    if not os.path.exists(visual_path):\n",
    "        return None\n",
    "    visual_features = read_feature_file(visual_path)\n",
    "    if visual_features is not None:\n",
    "        # 视觉特征现在是768维，原代码没裁剪\n",
    "        if len(visual_features) < 768:\n",
    "            visual_features = np.pad(visual_features, (0, 768 - len(visual_features)), 'constant')\n",
    "        elif len(visual_features) > 768:\n",
    "            visual_features = visual_features[:768]\n",
    "    return visual_features\n",
    "\n",
    "def extract_all_features(df, base_path=\"\"):\n",
    "    layoutxlm_features = []\n",
    "    visual_features = []\n",
    "    valid_indices = []\n",
    "    total = len(df)\n",
    "    valid_count = 0\n",
    "    print(f\"Processing {total} samples...\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_id = row['ID']\n",
    "        province = row['省份']\n",
    "\n",
    "        lxlm_feat = get_layoutxlm_features(product_id, province, base_path)\n",
    "        vis_feat = get_visual_features(product_id, province, base_path)\n",
    "\n",
    "        if lxlm_feat is not None and vis_feat is not None:\n",
    "            layoutxlm_features.append(lxlm_feat)\n",
    "            visual_features.append(vis_feat)\n",
    "            valid_indices.append(idx)\n",
    "            valid_count += 1\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"Processed {idx + 1}/{total} samples, {valid_count} valid features found\")\n",
    "\n",
    "    print(f\"\\nFeature extraction completed:\")\n",
    "    print(f\"Total samples: {total}\")\n",
    "    print(f\"Valid samples: {valid_count}\")\n",
    "    print(f\"Invalid samples: {total - valid_count}\")\n",
    "\n",
    "    if not layoutxlm_features or not visual_features:\n",
    "        raise ValueError(\"No valid features found\")\n",
    "\n",
    "    layoutxlm_matrix = np.stack(layoutxlm_features)\n",
    "    visual_matrix = np.stack(visual_features)\n",
    "\n",
    "    # 处理NaN\n",
    "    for arr in [layoutxlm_matrix, visual_matrix]:\n",
    "        nan_mask = np.isnan(arr)\n",
    "        if nan_mask.any():\n",
    "            for col in range(arr.shape[1]):\n",
    "                col_mean = np.nanmean(arr[:, col])\n",
    "                arr[:, col] = np.nan_to_num(arr[:, col], nan=col_mean)\n",
    "\n",
    "    # 去除常量列\n",
    "    var_lxlm = np.var(layoutxlm_matrix, axis=0)\n",
    "    non_constant_cols_lxlm = var_lxlm > 1e-6\n",
    "    layoutxlm_matrix = layoutxlm_matrix[:, non_constant_cols_lxlm]\n",
    "    print(f\"Removed {sum(~non_constant_cols_lxlm)} constant features from layoutxlm\")\n",
    "\n",
    "    var_vis = np.var(visual_matrix, axis=0)\n",
    "    non_constant_cols_vis = var_vis > 1e-6\n",
    "    visual_matrix = visual_matrix[:, non_constant_cols_vis]\n",
    "    print(f\"Removed {sum(~non_constant_cols_vis)} constant features from visual\")\n",
    "\n",
    "    # 标准化\n",
    "    scaler_lxlm = StandardScaler()\n",
    "    layoutxlm_matrix = scaler_lxlm.fit_transform(layoutxlm_matrix)\n",
    "\n",
    "    scaler_vis = StandardScaler()\n",
    "    visual_matrix = scaler_vis.fit_transform(visual_matrix)\n",
    "\n",
    "    return layoutxlm_matrix, visual_matrix, valid_indices\n",
    "\n",
    "# ----------- 数据预处理 -----------\n",
    "\n",
    "# \n",
    "def prepare_baseline_only(df):\n",
    "    numeric_features = ['价格', '评分', '点评数']\n",
    "    binary_features = ['无购物', '无自费', '成团保障', '退改政策', '是否促销']\n",
    "\n",
    "    X_numeric = df[numeric_features].copy()\n",
    "    X_numeric['价格'] = X_numeric['价格'].fillna(X_numeric['价格'].mean())\n",
    "    X_numeric['评分'] = X_numeric['评分'].fillna(X_numeric['评分'].mean())\n",
    "    X_numeric['点评数'] = X_numeric['点评数'].fillna(X_numeric['点评数'].mean())\n",
    "    X_numeric['价格'] = np.log1p(X_numeric['价格'])\n",
    "    X_numeric['点评数'] = np.log1p(X_numeric['点评数'])\n",
    "\n",
    "    X_binary = df[binary_features].fillna(0).astype(float)\n",
    "\n",
    "    X = pd.concat([X_numeric, X_binary], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0)\n",
    "    return X_scaled\n",
    "\n",
    "def prepare_baseline_data(df):\n",
    "    numeric_features = ['价格', '评分', '点评数']\n",
    "    binary_features = ['无购物', '无自费', '成团保障', '退改政策', '是否促销']\n",
    "    index_features = ['index_6', 'index_7', 'index_8', 'index_9', 'index_10']\n",
    "\n",
    "    X_numeric = df[numeric_features].copy()\n",
    "    X_numeric['价格'] = X_numeric['价格'].fillna(X_numeric['价格'].mean())\n",
    "    X_numeric['评分'] = X_numeric['评分'].fillna(X_numeric['评分'].mean())\n",
    "    X_numeric['点评数'] = X_numeric['点评数'].fillna(X_numeric['点评数'].mean())\n",
    "    X_numeric['价格'] = np.log1p(X_numeric['价格'])\n",
    "    X_numeric['点评数'] = np.log1p(X_numeric['点评数'])\n",
    "\n",
    "    X_binary = df[binary_features].fillna(0).astype(float)\n",
    "    X_index = df[index_features].copy()\n",
    "    X_index = X_index.fillna(X_index.mean())\n",
    "    X_index = np.log1p(X_index)\n",
    "\n",
    "    X = pd.concat([X_numeric, X_binary, X_index], axis=1)\n",
    "    # 保留标准化或取消，若保留，需要告诉模型输入维度13\n",
    "    # 这里保持现有标准化逻辑：\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0)\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "def prepare_data_combined(df, base_path=\"\", use_layoutxlm=True, use_visual=True):\n",
    "    \n",
    "    X_basic = prepare_baseline_data(df)\n",
    "    basic_dim = X_basic.shape[1]  # 64维左右\n",
    "\n",
    "    \n",
    "    layoutxlm_features, visual_features, valid_indices = extract_all_features(df, base_path)\n",
    "\n",
    "  \n",
    "    df_valid = df.iloc[valid_indices].reset_index(drop=True)\n",
    "    X_basic_valid = X_basic[valid_indices]\n",
    "\n",
    "\n",
    "    features_list = [X_basic_valid]\n",
    "    if use_layoutxlm:\n",
    "        features_list.append(layoutxlm_features)\n",
    "    if use_visual:\n",
    "        features_list.append(visual_features)\n",
    "\n",
    "    X_final = np.concatenate(features_list, axis=1)\n",
    "    return X_final, df_valid, basic_dim, layoutxlm_features.shape[1], visual_features.shape[1]\n",
    "\n",
    "def prepare_targets(df):\n",
    "    sales_columns = ['6月月销量', '7月月销量', '8月月销量', '9月月销量', '10月月销量']\n",
    "    sales_data = df[sales_columns].fillna(0).values\n",
    "    sales_data = np.clip(sales_data, a_min=0, a_max=None)\n",
    "    sales_data = np.log1p(sales_data)\n",
    "    sales_data = np.nan_to_num(sales_data, nan=0)\n",
    "    return sales_data\n",
    "\n",
    "# ----------- 数据集与模型 -----------\n",
    "\n",
    "class TourismDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.FloatTensor(features.astype(np.float32))\n",
    "        self.targets = torch.FloatTensor(targets.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "class HuberMSELoss(nn.Module):\n",
    "    def __init__(self, delta=1.0, alpha=0.7):\n",
    "        super(HuberMSELoss, self).__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.mse(pred, target) + (1 - self.alpha) * self.huber(pred, target)\n",
    "\n",
    "class FeatureFusionMLP(nn.Module):\n",
    "    def __init__(self, basic_dim, layoutxlm_dim, visual_dim, reduced_dim=64):\n",
    "        super(FeatureFusionMLP, self).__init__()\n",
    "        self.reduced_dim = reduced_dim\n",
    "\n",
    "      \n",
    "        self.basic_reduce = nn.Linear(basic_dim, reduced_dim)\n",
    "        self.layoutxlm_reduce = nn.Linear(layoutxlm_dim, reduced_dim) if layoutxlm_dim is not None else None\n",
    "        self.visual_reduce = nn.Linear(visual_dim, reduced_dim) if visual_dim is not None else None\n",
    "\n",
    "        input_dim = reduced_dim * 3  # \n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),  \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        start = 0\n",
    "        basic_feat = x[:, start:start + self.basic_reduce.in_features]\n",
    "        start += self.basic_reduce.in_features\n",
    "\n",
    "        lxlm_feat = None\n",
    "        if self.layoutxlm_reduce:\n",
    "            lxlm_feat = x[:, start:start + self.layoutxlm_reduce.in_features]\n",
    "            start += self.layoutxlm_reduce.in_features\n",
    "\n",
    "        visual_feat = None\n",
    "        if self.visual_reduce:\n",
    "            visual_feat = x[:, start:start + self.visual_reduce.in_features]\n",
    "\n",
    "       \n",
    "        basic_reduced = self.basic_reduce(basic_feat)\n",
    "        lxlm_reduced = self.layoutxlm_reduce(lxlm_feat) if lxlm_feat is not None else torch.zeros_like(basic_reduced)\n",
    "        visual_reduced = self.visual_reduce(visual_feat) if visual_feat is not None else torch.zeros_like(basic_reduced)\n",
    "\n",
    "        fused = torch.cat([basic_reduced, lxlm_reduced, visual_reduced], dim=1)\n",
    "        out = self.net(fused)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, test_loader, device, epochs=100):\n",
    "    criterion = HuberMSELoss(delta=1.0, alpha=0.7)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4, epochs=epochs, steps_per_epoch=len(train_loader),pct_start=0.1,\n",
    "        anneal_strategy='cos')\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets.view(-1,1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, targets.view(-1,1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions).reshape(-1)\n",
    "    actuals = np.array(actuals).reshape(-1)\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "\n",
    "def run_compare_five_models(data_path):\n",
    "    print(\"读取数据...\")\n",
    "    df = pd.read_excel(data_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  \n",
    "    print(\"提取统一样本子集（layoutxlm+视觉）...\")\n",
    "    layoutxlm_features, visual_features, valid_indices = extract_all_features(df)\n",
    "    df_common = df.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "  \n",
    "    print(\"准备Baseline数据...\")\n",
    "    X0 = prepare_baseline_only(df_common)\n",
    "    y = prepare_targets(df_common)\n",
    "    y = y.reshape(-1, 1)\n",
    "    X0_repeat = np.repeat(X0, 5, axis=0)\n",
    "    y_repeat = y.reshape(-1)\n",
    "\n",
    "   \n",
    "    print(\"准备Baseline+百度数据...\")\n",
    "    X1 = prepare_baseline_data(df_common)\n",
    "    X1_repeat = np.repeat(X1, 5, axis=0)\n",
    "\n",
    "    print(\"准备Baseline+百度+LayoutXLM数据...\")\n",
    "    X2 = np.concatenate([X1, layoutxlm_features], axis=1)\n",
    "    X2_repeat = np.repeat(X2, 5, axis=0)\n",
    "\n",
    "    print(\"准备Baseline+百度+LayoutXLM+视觉数据...\")\n",
    "    X3 = np.concatenate([X1, layoutxlm_features, visual_features], axis=1)\n",
    "    X3_repeat = np.repeat(X3, 5, axis=0)\n",
    "\n",
    "    print(\"准备Baseline+百度+视觉数据...\")\n",
    "    X4 = np.concatenate([X1, visual_features], axis=1)\n",
    "    X4_repeat = np.repeat(X4, 5, axis=0)\n",
    "\n",
    "\n",
    "    basic_dim = X1.shape[1]\n",
    "    layoutxlm_dim = layoutxlm_features.shape[1]\n",
    "    visual_dim = visual_features.shape[1]\n",
    "\n",
    "   \n",
    "    results = {}\n",
    "    for X, name in zip(\n",
    "        [X0_repeat, X1_repeat, X2_repeat, X3_repeat, X4_repeat],\n",
    "        ['Baseline',\n",
    "         'Baseline+百度',\n",
    "         'Baseline+百度+LayoutXLM',\n",
    "         'Baseline+百度+LayoutXLM+视觉',\n",
    "         'Baseline+百度+视觉']):\n",
    "\n",
    "        print(f\"\\n训练模型: {name}\")\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_repeat, test_size=0.2, random_state=42)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "        batch_size = min(32, len(X_train))\n",
    "        train_loader = DataLoader(TourismDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(TourismDataset(X_val, y_val), batch_size=batch_size)\n",
    "        test_loader = DataLoader(TourismDataset(X_test, y_test), batch_size=batch_size)\n",
    "\n",
    "        if name == 'Baseline':\n",
    "            model = FeatureFusionMLP(X0.shape[1], 0, 0).to(device)\n",
    "        elif name == 'Baseline+百度':\n",
    "            model = FeatureFusionMLP(basic_dim, 0, 0).to(device)\n",
    "        elif name == 'Baseline+百度+LayoutXLM':\n",
    "            model = FeatureFusionMLP(basic_dim, layoutxlm_dim, 0).to(device)\n",
    "        elif name == 'Baseline+百度+LayoutXLM+视觉':\n",
    "            model = FeatureFusionMLP(basic_dim, layoutxlm_dim, visual_dim).to(device)\n",
    "        else:  # Baseline+百度+视觉\n",
    "            model = FeatureFusionMLP(basic_dim, 0, visual_dim).to(device)\n",
    "\n",
    "        res = train_eval(model, train_loader, val_loader, test_loader, device)\n",
    "        results[name] = res\n",
    "\n",
    "\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"{name:<30} {metrics['MSE']:10.4f} {metrics['RMSE']:10.4f} {metrics['R2']:10.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = r\"\"\n",
    "    run_compare_five_models(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd749ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "def read_feature_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip()\n",
    "            content = content.strip('[]')\n",
    "            features = [float(x.strip()) for x in content.split(',')]\n",
    "            return np.array(features)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_layoutxlm_features(product_id, province, base_path):\n",
    "    product_path = os.path.join(base_path, province, str(product_id), '')\n",
    "    if not os.path.exists(product_path):\n",
    "        return None\n",
    "    cls_path = os.path.join(product_path, '')\n",
    "    img_path = os.path.join(product_path, '')\n",
    "    cls_features = read_feature_file(cls_path)\n",
    "    img_features = read_feature_file(img_path)\n",
    "    if cls_features is None or img_features is None:\n",
    "        return None\n",
    "    return np.concatenate([cls_features, img_features])\n",
    "\n",
    "def get_visual_features(product_id, province, base_path):\n",
    "    visual_path = os.path.join(base_path, province, str(product_id), '', '')\n",
    "    if not os.path.exists(visual_path):\n",
    "        return None\n",
    "    visual_features = read_feature_file(visual_path)\n",
    "    if visual_features is not None:\n",
    "        \n",
    "        if len(visual_features) < 768:\n",
    "            visual_features = np.pad(visual_features, (0, 768 - len(visual_features)), 'constant')\n",
    "        elif len(visual_features) > 768:\n",
    "            visual_features = visual_features[:768]\n",
    "    return visual_features\n",
    "\n",
    "def get_nonvisual_features(product_id, province, base_path):\n",
    "    nonvisual_path = os.path.join(base_path, province, str(product_id), '', '')\n",
    "    if not os.path.exists(nonvisual_path):\n",
    "        return None\n",
    "    nonvisual_features = read_feature_file(nonvisual_path)\n",
    "    if nonvisual_features is not None:\n",
    "        \n",
    "        if len(nonvisual_features) < 768:\n",
    "            nonvisual_features = np.pad(nonvisual_features, (0, 768 - len(nonvisual_features)), 'constant')\n",
    "        elif len(nonvisual_features) > 768:\n",
    "            nonvisual_features = nonvisual_features[:768]\n",
    "    return nonvisual_features\n",
    "\n",
    "def extract_all_features(df, base_path=\"\"):\n",
    "    layoutxlm_features = []\n",
    "    visual_features = []\n",
    "    nonvisual_features = []\n",
    "    valid_indices = []\n",
    "    total = len(df)\n",
    "    valid_count = 0\n",
    "    print(f\"Processing {total} samples...\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_id = row['ID']\n",
    "        province = row['省份']\n",
    "\n",
    "        lxlm_feat = get_layoutxlm_features(product_id, province, base_path)\n",
    "        vis_feat = get_visual_features(product_id, province, base_path)\n",
    "        nonvis_feat = get_nonvisual_features(product_id, province, base_path)\n",
    "\n",
    "        if lxlm_feat is not None and vis_feat is not None and nonvis_feat is not None:\n",
    "            layoutxlm_features.append(lxlm_feat)\n",
    "            visual_features.append(vis_feat)\n",
    "            nonvisual_features.append(nonvis_feat)\n",
    "            valid_indices.append(idx)\n",
    "            valid_count += 1\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"Processed {idx + 1}/{total} samples, {valid_count} valid features found\")\n",
    "\n",
    "    \n",
    "\n",
    "    if not layoutxlm_features or not visual_features or not nonvisual_features:\n",
    "        raise ValueError(\"No valid features found\")\n",
    "\n",
    "    layoutxlm_matrix = np.stack(layoutxlm_features)\n",
    "    visual_matrix = np.stack(visual_features)\n",
    "    nonvisual_matrix = np.stack(nonvisual_features)\n",
    "\n",
    "    \n",
    "    for arr in [layoutxlm_matrix, visual_matrix, nonvisual_matrix]:\n",
    "        nan_mask = np.isnan(arr)\n",
    "        if nan_mask.any():\n",
    "            for col in range(arr.shape[1]):\n",
    "                col_mean = np.nanmean(arr[:, col])\n",
    "                arr[:, col] = np.nan_to_num(arr[:, col], nan=col_mean)\n",
    "\n",
    "    # 去除常量列\n",
    "    var_lxlm = np.var(layoutxlm_matrix, axis=0)\n",
    "    non_constant_cols_lxlm = var_lxlm > 1e-6\n",
    "    layoutxlm_matrix = layoutxlm_matrix[:, non_constant_cols_lxlm]\n",
    "    print(f\"Removed {sum(~non_constant_cols_lxlm)} constant features from layoutxlm\")\n",
    "\n",
    "    var_vis = np.var(visual_matrix, axis=0)\n",
    "    non_constant_cols_vis = var_vis > 1e-6\n",
    "    visual_matrix = visual_matrix[:, non_constant_cols_vis]\n",
    "    print(f\"Removed {sum(~non_constant_cols_vis)} constant features from visual\")\n",
    "\n",
    "    var_nonvis = np.var(nonvisual_matrix, axis=0)\n",
    "    non_constant_cols_nonvis = var_nonvis > 1e-6\n",
    "    nonvisual_matrix = nonvisual_matrix[:, non_constant_cols_nonvis]\n",
    "    print(f\"Removed {sum(~non_constant_cols_nonvis)} constant features from nonvisual\")\n",
    "\n",
    "\n",
    "    scaler_lxlm = StandardScaler()\n",
    "    layoutxlm_matrix = scaler_lxlm.fit_transform(layoutxlm_matrix)\n",
    "\n",
    "    scaler_vis = StandardScaler()\n",
    "    visual_matrix = scaler_vis.fit_transform(visual_matrix)\n",
    "\n",
    "    scaler_nonvis = StandardScaler()\n",
    "    nonvisual_matrix = scaler_nonvis.fit_transform(nonvisual_matrix)\n",
    "\n",
    "    return layoutxlm_matrix, visual_matrix, nonvisual_matrix, valid_indices\n",
    "\n",
    "\n",
    "\n",
    "def prepare_baseline_data(df):\n",
    "    numeric_features = ['价格', '评分', '点评数']\n",
    "    binary_features = ['无购物', '无自费', '成团保障', '退改政策', '是否促销']\n",
    "    index_features = ['index_6', 'index_7', 'index_8', 'index_9', 'index_10']\n",
    "\n",
    "    X_numeric = df[numeric_features].copy()\n",
    "    X_numeric['价格'] = X_numeric['价格'].fillna(X_numeric['价格'].mean())\n",
    "    X_numeric['评分'] = X_numeric['评分'].fillna(X_numeric['评分'].mean())\n",
    "    X_numeric['点评数'] = X_numeric['点评数'].fillna(X_numeric['点评数'].mean())\n",
    "    X_numeric['价格'] = np.log1p(X_numeric['价格'])\n",
    "    X_numeric['点评数'] = np.log1p(X_numeric['点评数'])\n",
    "\n",
    "    X_binary = df[binary_features].fillna(0).astype(float)\n",
    "    X_index = df[index_features].copy()\n",
    "    X_index = X_index.fillna(X_index.mean())\n",
    "    X_index = np.log1p(X_index)\n",
    "\n",
    "    X = pd.concat([X_numeric, X_binary, X_index], axis=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled = np.nan_to_num(X_scaled, nan=0)\n",
    "    return X_scaled\n",
    "\n",
    "def prepare_targets(df):\n",
    "    sales_columns = ['6月月销量', '7月月销量', '8月月销量', '9月月销量', '10月月销量']\n",
    "    sales_data = df[sales_columns].fillna(0).values\n",
    "    sales_data = np.clip(sales_data, a_min=0, a_max=None)\n",
    "    sales_data = np.log1p(sales_data)\n",
    "    sales_data = np.nan_to_num(sales_data, nan=0)\n",
    "    return sales_data\n",
    "\n",
    "\n",
    "class TourismDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = torch.FloatTensor(features.astype(np.float32))\n",
    "        self.targets = torch.FloatTensor(targets.astype(np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]\n",
    "\n",
    "class HuberMSELoss(nn.Module):\n",
    "    def __init__(self, delta=1.0, alpha=0.7):\n",
    "        super(HuberMSELoss, self).__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.mse(pred, target) + (1 - self.alpha) * self.huber(pred, target)\n",
    "\n",
    "class FeatureFusionMLP(nn.Module):\n",
    "    def __init__(self, basic_dim, layoutxlm_dim=None, visual_dim=None, nonvisual_dim=None, reduced_dim=64):\n",
    "        super(FeatureFusionMLP, self).__init__()\n",
    "        self.reduced_dim = reduced_dim\n",
    "\n",
    "       \n",
    "        self.basic_reduce = nn.Linear(basic_dim, reduced_dim)\n",
    "        self.layoutxlm_reduce = nn.Linear(layoutxlm_dim, reduced_dim) if layoutxlm_dim is not None else None\n",
    "        self.visual_reduce = nn.Linear(visual_dim, reduced_dim) if visual_dim is not None else None\n",
    "        self.nonvisual_reduce = nn.Linear(nonvisual_dim, reduced_dim) if nonvisual_dim is not None else None\n",
    "\n",
    "        \n",
    "        input_dim = reduced_dim\n",
    "        if self.layoutxlm_reduce:\n",
    "            input_dim += reduced_dim\n",
    "        if self.visual_reduce:\n",
    "            input_dim += reduced_dim\n",
    "        if self.nonvisual_reduce:\n",
    "            input_dim += reduced_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        start = 0\n",
    "        basic_feat = x[:, start:start + self.basic_reduce.in_features]\n",
    "        start += self.basic_reduce.in_features\n",
    "\n",
    "        lxlm_feat = None\n",
    "        if self.layoutxlm_reduce:\n",
    "            lxlm_feat = x[:, start:start + self.layoutxlm_reduce.in_features]\n",
    "            start += self.layoutxlm_reduce.in_features\n",
    "\n",
    "        visual_feat = None\n",
    "        if self.visual_reduce:\n",
    "            visual_feat = x[:, start:start + self.visual_reduce.in_features]\n",
    "            start += self.visual_reduce.in_features\n",
    "\n",
    "        nonvisual_feat = None\n",
    "        if self.nonvisual_reduce:\n",
    "            nonvisual_feat = x[:, start:start + self.nonvisual_reduce.in_features]\n",
    "\n",
    "      \n",
    "        basic_reduced = self.basic_reduce(basic_feat)\n",
    "        lxlm_reduced = self.layoutxlm_reduce(lxlm_feat) if lxlm_feat is not None else torch.zeros_like(basic_reduced)\n",
    "        visual_reduced = self.visual_reduce(visual_feat) if visual_feat is not None else torch.zeros_like(basic_reduced)\n",
    "        nonvisual_reduced = self.nonvisual_reduce(nonvisual_feat) if nonvisual_feat is not None else torch.zeros_like(basic_reduced)\n",
    "\n",
    "     \n",
    "        fused = [basic_reduced]\n",
    "        if lxlm_feat is not None:\n",
    "            fused.append(lxlm_reduced)\n",
    "        if visual_feat is not None:\n",
    "            fused.append(visual_reduced)\n",
    "        if nonvisual_feat is not None:\n",
    "            fused.append(nonvisual_reduced)\n",
    "        \n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        out = self.net(fused)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, test_loader, device, epochs=100):\n",
    "    criterion = HuberMSELoss(delta=1.0, alpha=0.7)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=5e-4, epochs=epochs, steps_per_epoch=len(train_loader),pct_start=0.1,\n",
    "        anneal_strategy='cos')\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets.view(-1,1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, targets.view(-1,1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "    predictions = np.array(predictions).reshape(-1)\n",
    "    actuals = np.array(actuals).reshape(-1)\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "\n",
    "\n",
    "def run_three_models_comparison(data_path):\n",
    "    print(\"读取数据...\")\n",
    "    df = pd.read_excel(data_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    " \n",
    "    print(\"提取统一样本子集（layoutxlm+视觉+非视觉）...\")\n",
    "    layoutxlm_features, visual_features, nonvisual_features, valid_indices = extract_all_features(df)\n",
    "    df_common = df.iloc[valid_indices].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    print(\"准备Baseline+百度特征...\")\n",
    "    X_basic = prepare_baseline_data(df_common)\n",
    "    basic_dim = X_basic.shape[1]\n",
    "    layoutxlm_dim = layoutxlm_features.shape[1]\n",
    "    visual_dim = visual_features.shape[1]\n",
    "    nonvisual_dim = nonvisual_features.shape[1]\n",
    "\n",
    "\n",
    "    y = prepare_targets(df_common)\n",
    "    y_repeat = y.reshape(-1, 1)\n",
    "\n",
    " \n",
    "\n",
    "    X_model1 = np.concatenate([X_basic, layoutxlm_features, visual_features], axis=1)\n",
    "    X_model1_repeat = np.repeat(X_model1, 5, axis=0)\n",
    "    \n",
    "    # 划分数据集\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_model1_repeat, y_repeat, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "    \n",
    "    batch_size = min(32, len(X_train))\n",
    "    train_loader = DataLoader(TourismDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TourismDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TourismDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    model1 = FeatureFusionMLP(basic_dim, layoutxlm_dim, visual_dim).to(device)\n",
    "    res1 = train_eval(model1, train_loader, val_loader, test_loader, device)\n",
    "\n",
    "\n",
    "    print(\"\\n训练模型2: Baseline+百度+LayoutXLM+视觉+非视觉\")\n",
    "    X_model2 = np.concatenate([X_basic, layoutxlm_features, visual_features, nonvisual_features], axis=1)\n",
    "    X_model2_repeat = np.repeat(X_model2, 5, axis=0)\n",
    "    \n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_model2_repeat, y_repeat, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(TourismDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TourismDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TourismDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    model2 = FeatureFusionMLP(basic_dim, layoutxlm_dim, visual_dim, nonvisual_dim).to(device)\n",
    "    res2 = train_eval(model2, train_loader, val_loader, test_loader, device)\n",
    "\n",
    "\n",
    "    print(\"\\n训练模型3: Baseline+百度+LayoutXLM+非视觉\")\n",
    "    X_model3 = np.concatenate([X_basic, layoutxlm_features, nonvisual_features], axis=1)\n",
    "    X_model3_repeat = np.repeat(X_model3, 5, axis=0)\n",
    "    \n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X_model3_repeat, y_repeat, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_loader = DataLoader(TourismDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TourismDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TourismDataset(X_test, y_test), batch_size=batch_size)\n",
    "    \n",
    "    model3 = FeatureFusionMLP(basic_dim, layoutxlm_dim, None, nonvisual_dim).to(device)\n",
    "    res3 = train_eval(model3, train_loader, val_loader, test_loader, device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = r\"\"\n",
    "    run_three_models_comparison(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70779097",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ========= 基础 I/O =========\n",
    "\n",
    "def read_feature_file(file_path: str) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip().strip('[]')\n",
    "            if not content:\n",
    "                return None\n",
    "            arr = np.array([float(x.strip()) for x in content.split(',')], dtype=float)\n",
    "            return arr.ravel()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def find_sensory_dir(base_path: str, province: str, product_id: str) -> Optional[str]:\n",
    "    prod_dir = os.path.join(base_path, str(province), str(product_id))\n",
    "    if not os.path.isdir(prod_dir):\n",
    "        return None\n",
    "    exact = os.path.join(prod_dir, \"\")\n",
    "    if os.path.isdir(exact):\n",
    "        return exact\n",
    "    # 兜底：名称包含“感官”的子目录\n",
    "    for name in os.listdir(prod_dir):\n",
    "        p = os.path.join(prod_dir, name)\n",
    "        if os.path.isdir(p) and (\"感官\" in name):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# ========= 特征读取 =========\n",
    "\n",
    "def get_layoutxlm_features(product_id, province, base_path) -> Optional[np.ndarray]:\n",
    "    product_path = os.path.join(base_path, str(province), str(product_id), '')\n",
    "    if not os.path.exists(product_path):\n",
    "        return None\n",
    "    cls_path = os.path.join(product_path, '')\n",
    "    img_path = os.path.join(product_path, '')\n",
    "    cls_features = read_feature_file(cls_path)\n",
    "    img_features = read_feature_file(img_path)\n",
    "    if cls_features is None or img_features is None:\n",
    "        return None\n",
    "    return np.concatenate([cls_features, img_features], axis=0)\n",
    "\n",
    "def _to_768(vec: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "    if vec is None:\n",
    "        return None\n",
    "    v = vec\n",
    "    if len(v) < 768:\n",
    "        v = np.pad(v, (0, 768 - len(v)), mode='constant')\n",
    "    elif len(v) > 768:\n",
    "        v = v[:768]\n",
    "    return v\n",
    "\n",
    "def get_visual_mean_vector(product_id, province, base_path, filename=\"\") -> Optional[np.ndarray]:\n",
    "    sdir = find_sensory_dir(base_path, province, product_id)\n",
    "    if sdir is None:\n",
    "        return None\n",
    "    v = read_feature_file(os.path.join(sdir, filename))\n",
    "    return _to_768(v)\n",
    "\n",
    "def get_one_nonvisual_vector(product_id, province, base_path, filename=\"\") -> Optional[np.ndarray]:\n",
    "   \n",
    "    sdir = find_sensory_dir(base_path, province, product_id)\n",
    "    if sdir is None:\n",
    "        return None\n",
    "    v = read_feature_file(os.path.join(sdir, filename))\n",
    "    return _to_768(v)\n",
    "\n",
    "\n",
    "\n",
    "def extract_subset_for_A_B(df: pd.DataFrame, base_path=r\"\"\n",
    "                           ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[int]]:\n",
    "    lxlm_list, vis_mean_list, one_nonvis_list, valid_idx = [], [], [], []\n",
    "    total = len(df); valid = 0\n",
    "    print(f\".\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        pid = row['ID']; prov = row['省份']\n",
    "        lxlm = get_layoutxlm_features(pid, prov, base_path)\n",
    "        vis  = get_visual_mean_vector(pid, prov, base_path, filename=\"\")\n",
    "        one  = get_one_nonvisual_vector(pid, prov, base_path, filename=\"\")\n",
    "        if lxlm is not None and vis is not None and one is not None:\n",
    "            lxlm_list.append(lxlm); vis_mean_list.append(vis); one_nonvis_list.append(one)\n",
    "            valid_idx.append(idx); valid += 1\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  已处理 {idx+1}/{total}，有效 {valid}\")\n",
    "\n",
    "    if valid == 0:\n",
    "        raise ValueError(\"没有找到满足条件的样本。\")\n",
    "\n",
    "    lxlm_mat       = np.stack(lxlm_list)\n",
    "    vis_mean_mat   = np.stack(vis_mean_list)\n",
    "    one_nonvis_mat = np.stack(one_nonvis_list)\n",
    "    print(f\"完成：有效样本 {valid} / {total}\")\n",
    "    return lxlm_mat, vis_mean_mat, one_nonvis_mat, valid_idx\n",
    "\n",
    "# ========= 预处理：Baseline+百度 / 目标 =========\n",
    "\n",
    "def prepare_baseline_data(df: pd.DataFrame) -> np.ndarray:\n",
    "    numeric_features = ['价格', '评分', '点评数']\n",
    "    binary_features  = ['无购物', '无自费', '成团保障', '退改政策', '是否促销']\n",
    "    index_features   = ['index_6', 'index_7', 'index_8', 'index_9', 'index_10']\n",
    "\n",
    "    Xn = df[numeric_features].copy()\n",
    "    Xn['价格'] = np.log1p(Xn['价格'].fillna(Xn['价格'].mean()))\n",
    "    Xn['评分'] = Xn['评分'].fillna(Xn['评分'].mean())\n",
    "    Xn['点评数'] = np.log1p(Xn['点评数'].fillna(Xn['点评数'].mean()))\n",
    "\n",
    "    Xb = df[binary_features].fillna(0).astype(float)\n",
    "    Xi = np.log1p(df[index_features].copy().fillna(df[index_features].mean()))\n",
    "\n",
    "    X = pd.concat([Xn, Xb, Xi], axis=1)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "def prepare_targets(df: pd.DataFrame) -> np.ndarray:\n",
    "    cols = ['6月月销量', '7月月销量', '8月月销量', '9月月销量', '10月月销量']\n",
    "    y = df[cols].fillna(0).values\n",
    "    y = np.clip(y, a_min=0, a_max=None)\n",
    "    y = np.log1p(y)\n",
    "    return np.nan_to_num(y, nan=0.0)\n",
    "\n",
    "# ========= 清洗：NaN/常量列/标准化 =========\n",
    "\n",
    "def clean_matrix(mat: np.ndarray, name: str) -> np.ndarray:\n",
    "    # NaN -> 列均值\n",
    "    if np.isnan(mat).any():\n",
    "        col_means = np.nanmean(mat, axis=0)\n",
    "        inds = np.where(np.isnan(mat))\n",
    "        mat[inds] = np.take(col_means, inds[1])\n",
    "    # 去常量列\n",
    "    var = np.var(mat, axis=0)\n",
    "    keep = var > 1e-6\n",
    "    dropped = int((~keep).sum())\n",
    "    if dropped:\n",
    "        print(f\"{name}: 移除常量列 {dropped} 个\")\n",
    "    mat = mat[:, keep] if keep.any() else mat\n",
    "    # 标准化\n",
    "    mat = StandardScaler().fit_transform(mat)\n",
    "    return mat\n",
    "\n",
    "# ========= 数据集 & 模型 =========\n",
    "\n",
    "class TourismDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "class HuberMSELoss(nn.Module):\n",
    "    def __init__(self, delta=1.0, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "        self.mse   = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.mse(pred, target) + (1 - self.alpha) * self.huber(pred, target)\n",
    "\n",
    "class FeatureFusionMLP(nn.Module):\n",
    "    def __init__(self, basic_dim, layoutxlm_dim=None, visual_dim=None, nonvisual_dim=None, reduced_dim=64):\n",
    "        super().__init__()\n",
    "        self.basic_reduce     = nn.Linear(basic_dim, reduced_dim)\n",
    "        self.layoutxlm_reduce = nn.Linear(layoutxlm_dim, reduced_dim) if layoutxlm_dim else None\n",
    "        self.visual_reduce    = nn.Linear(visual_dim, reduced_dim)    if visual_dim else None\n",
    "        self.nonvisual_reduce = nn.Linear(nonvisual_dim, reduced_dim) if nonvisual_dim else None\n",
    "\n",
    "        input_dim = reduced_dim\n",
    "        if self.layoutxlm_reduce: input_dim += reduced_dim\n",
    "        if self.visual_reduce:    input_dim += reduced_dim\n",
    "        if self.nonvisual_reduce: input_dim += reduced_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32), nn.ReLU(), nn.BatchNorm1d(32), nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16), nn.ReLU(), nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        start = 0\n",
    "        b_in = self.basic_reduce.in_features\n",
    "        basic = x[:, start:start+b_in]; start += b_in\n",
    "\n",
    "        lxlm = None\n",
    "        if self.layoutxlm_reduce:\n",
    "            l_in = self.layoutxlm_reduce.in_features\n",
    "            lxlm = x[:, start:start+l_in]; start += l_in\n",
    "\n",
    "        vis = None\n",
    "        if self.visual_reduce:\n",
    "            v_in = self.visual_reduce.in_features\n",
    "            vis = x[:, start:start+v_in]; start += v_in\n",
    "\n",
    "        nonvis = None\n",
    "        if self.nonvisual_reduce:\n",
    "            n_in = self.nonvisual_reduce.in_features\n",
    "            nonvis = x[:, start:start+n_in]\n",
    "\n",
    "        b = self.basic_reduce(basic)\n",
    "        l = self.layoutxlm_reduce(lxlm) if lxlm is not None else torch.zeros_like(b)\n",
    "        v = self.visual_reduce(vis)      if vis   is not None else torch.zeros_like(b)\n",
    "        n = self.nonvisual_reduce(nonvis)if nonvis is not None else torch.zeros_like(b)\n",
    "\n",
    "        feats = [b]\n",
    "        if lxlm is not None: feats.append(l)\n",
    "        if vis  is not None: feats.append(v)\n",
    "        if nonvis is not None: feats.append(n)\n",
    "        return self.net(torch.cat(feats, dim=1))\n",
    "\n",
    "# ========= 训练/验证/测试 =========\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, test_loader, device, epochs=100):\n",
    "    criterion = HuberMSELoss(delta=1.0, alpha=0.7)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optim, max_lr=5e-4, epochs=epochs, steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1, anneal_strategy='cos'\n",
    "    )\n",
    "    best_val, best_state, patience, cnt = float('inf'), None, 10, 0\n",
    "    for ep in range(epochs):\n",
    "        model.train(); tl = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optim.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb.view(-1,1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step(); sched.step()\n",
    "            tl += loss.item()\n",
    "        tl /= max(1, len(train_loader))\n",
    "\n",
    "        model.eval(); vl = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                vl += criterion(model(xb), yb.view(-1,1)).item()\n",
    "        vl /= max(1, len(val_loader))\n",
    "\n",
    "        if vl < best_val:\n",
    "            best_val, best_state, cnt = vl, model.state_dict(), 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "            if cnt >= patience:\n",
    "                print(f\"Early stopping @ epoch {ep+1}\")\n",
    "                break\n",
    "        if ep == 0 or (ep + 1) % 10 == 0:\n",
    "            print(f\"Epoch {ep+1:03d} | Train {tl:.4f} | Val {vl:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    model.eval(); preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pr = model(xb)\n",
    "            preds.append(pr.cpu().numpy()); gts.append(yb.cpu().numpy())\n",
    "    y_pred = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    y_true = np.concatenate(gts, axis=0).reshape(-1)\n",
    "    mse, rmse, r2 = mean_squared_error(y_true, y_pred), np.sqrt(mean_squared_error(y_true, y_pred)), r2_score(y_true, y_pred)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# ========= 主流程（模型A vs 模型B） =========\n",
    "\n",
    "def run_AB(data_path, base_path=r\"D:\\有效数据汇总\", epochs=100, seed=42):\n",
    "    np.random.seed(seed); torch.manual_seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"读取数据...\")\n",
    "    df_all = pd.read_excel(data_path)\n",
    "\n",
    "    print(\"抽取统一子集：LayoutXLM + 视觉均值向量.txt + 视觉+仅一种.txt\")\n",
    "    lxlm_mat, vis_mean_mat, one_nonvis_mat, valid_idx = extract_subset_for_A_B(df_all, base_path)\n",
    "    df = df_all.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    lxlm_mat       = clean_matrix(lxlm_mat,       \"LayoutXLM\")\n",
    "    vis_mean_mat   = clean_matrix(vis_mean_mat,   \"VisualMean\")\n",
    "    one_nonvis_mat = clean_matrix(one_nonvis_mat, \"OneNonVisual\")\n",
    "\n",
    "    \n",
    "    X_basic = prepare_baseline_data(df)\n",
    "    y_mat   = prepare_targets(df)    \n",
    "    y_flat  = y_mat.reshape(-1, 1)  \n",
    "\n",
    "    basic_dim     = X_basic.shape[1]\n",
    "    layoutxlm_dim = lxlm_mat.shape[1]\n",
    "    visual_dim    = vis_mean_mat.shape[1]\n",
    "    one_dim       = one_nonvis_mat.shape[1]\n",
    "\n",
    "  \n",
    "    X_A = np.concatenate([X_basic, lxlm_mat, vis_mean_mat], axis=1)         \n",
    "    X_B = np.concatenate([X_basic, lxlm_mat, one_nonvis_mat], axis=1)     \n",
    "    X_A_rep = np.repeat(X_A, 5, axis=0)\n",
    "    X_B_rep = np.repeat(X_B, 5, axis=0)\n",
    "\n",
    "   \n",
    "    Nrep = X_A_rep.shape[0]\n",
    "    all_idx = np.arange(Nrep)\n",
    "    idx_train_val, idx_test = train_test_split(all_idx, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    idx_train, idx_val = train_test_split(idx_train_val, test_size=0.2, random_state=seed, shuffle=True)\n",
    "\n",
    "    def make_loaders(Xrep):\n",
    "        X_train, X_val, X_test = Xrep[idx_train], Xrep[idx_val], Xrep[idx_test]\n",
    "        y_train, y_val, y_test = y_flat[idx_train], y_flat[idx_val], y_flat[idx_test]\n",
    "        bs = min(32, max(1, len(idx_train)//8)) if len(idx_train) > 0 else 16\n",
    "        bs = max(8, min(64, bs))\n",
    "        return (\n",
    "            DataLoader(TourismDataset(X_train, y_train), batch_size=bs, shuffle=True),\n",
    "            DataLoader(TourismDataset(X_val,   y_val),   batch_size=bs),\n",
    "            DataLoader(TourismDataset(X_test,  y_test),  batch_size=bs),\n",
    "        )\n",
    "\n",
    "    trA, vaA, teA = make_loaders(X_A_rep)\n",
    "    trB, vaB, teB = make_loaders(X_B_rep)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = r\"\"\n",
    "    run_AB(data_path, base_path=r\"\", epochs=100, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208dd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "def read_feature_file(file_path: str) -> Optional[np.ndarray]:\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip().strip('[]')\n",
    "            if not content:\n",
    "                return None\n",
    "            arr = np.array([float(x.strip()) for x in content.split(',')], dtype=float)\n",
    "            return arr.ravel()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def find_sensory_dir(base_path: str, province: str, product_id: str) -> Optional[str]:\n",
    "    prod_dir = os.path.join(base_path, str(province), str(product_id))\n",
    "    if not os.path.isdir(prod_dir):\n",
    "        return None\n",
    "    exact = os.path.join(prod_dir, \"\")\n",
    "    if os.path.isdir(exact):\n",
    "        return exact\n",
    "    # 兜底：名称包含“感官”的子目录\n",
    "    for name in os.listdir(prod_dir):\n",
    "        p = os.path.join(prod_dir, name)\n",
    "        if os.path.isdir(p) and (\"\" in name):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# ========= 特征读取 =========\n",
    "\n",
    "def get_layoutxlm_features(product_id, province, base_path) -> Optional[np.ndarray]:\n",
    "    product_path = os.path.join(base_path, str(province), str(product_id), '')\n",
    "    if not os.path.exists(product_path):\n",
    "        return None\n",
    "    cls_path = os.path.join(product_path, '')\n",
    "    img_path = os.path.join(product_path, '')\n",
    "    cls_features = read_feature_file(cls_path)\n",
    "    img_features = read_feature_file(img_path)\n",
    "    if cls_features is None or img_features is None:\n",
    "        return None\n",
    "    return np.concatenate([cls_features, img_features], axis=0)\n",
    "\n",
    "def _to_768(vec: Optional[np.ndarray]) -> Optional[np.ndarray]:\n",
    "    if vec is None:\n",
    "        return None\n",
    "    v = vec\n",
    "    if len(v) < 768:\n",
    "        v = np.pad(v, (0, 768 - len(v)), mode='constant')\n",
    "    elif len(v) > 768:\n",
    "        v = v[:768]\n",
    "    return v\n",
    "\n",
    "def get_visual_mean_vector(product_id, province, base_path, filename=\"\") -> Optional[np.ndarray]:\n",
    "    sdir = find_sensory_dir(base_path, province, product_id)\n",
    "    if sdir is None:\n",
    "        return None\n",
    "    v = read_feature_file(os.path.join(sdir, filename))\n",
    "    return _to_768(v)\n",
    "\n",
    "def get_two_or_more_vector(product_id, province, base_path, filename=\"\") -> Optional[np.ndarray]:\n",
    "    # 直接按 768 维处理，和视觉均值向量一致\n",
    "    sdir = find_sensory_dir(base_path, province, product_id)\n",
    "    if sdir is None:\n",
    "        return None\n",
    "    v = read_feature_file(os.path.join(sdir, filename))\n",
    "    return _to_768(v)\n",
    "\n",
    "# ========= 子集抽取：确保三者同时存在 =========\n",
    "\n",
    "def extract_subset_for_A_C(df: pd.DataFrame, base_path=r\"\"\n",
    "                           ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, List[int]]:\n",
    "    lxlm_list, vis_mean_list, two_or_more_list, valid_idx = [], [], [], []\n",
    "    total = len(df); valid = 0\n",
    "    \n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        pid = row['ID']; prov = row['省份']\n",
    "        lxlm = get_layoutxlm_features(pid, prov, base_path)\n",
    "        vis  = get_visual_mean_vector(pid, prov, base_path, filename=\"\")\n",
    "        two  = get_two_or_more_vector(pid, prov, base_path, filename=\"\")\n",
    "        if lxlm is not None and vis is not None and two is not None:\n",
    "            lxlm_list.append(lxlm); vis_mean_list.append(vis); two_or_more_list.append(two)\n",
    "            valid_idx.append(idx); valid += 1\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"  已处理 {idx+1}/{total}，有效 {valid}\")\n",
    "\n",
    "    if valid == 0:\n",
    "        raise ValueError(\"没有找到满足条件的样本。\")\n",
    "\n",
    "    lxlm_mat       = np.stack(lxlm_list)\n",
    "    vis_mean_mat   = np.stack(vis_mean_list)\n",
    "    two_or_more_mat= np.stack(two_or_more_list)\n",
    "    print(f\"完成：有效样本 {valid} / {total}\")\n",
    "    return lxlm_mat, vis_mean_mat, two_or_more_mat, valid_idx\n",
    "\n",
    "# ========= 预处理：Baseline+百度 / 目标 =========\n",
    "\n",
    "def prepare_baseline_data(df: pd.DataFrame) -> np.ndarray:\n",
    "    numeric_features = ['价格', '评分', '点评数']\n",
    "    binary_features  = ['无购物', '无自费', '成团保障', '退改政策', '是否促销']\n",
    "    index_features   = ['index_6', 'index_7', 'index_8', 'index_9', 'index_10']\n",
    "\n",
    "    Xn = df[numeric_features].copy()\n",
    "    Xn['价格']   = np.log1p(Xn['价格'].fillna(Xn['价格'].mean()))\n",
    "    Xn['评分']   = Xn['评分'].fillna(Xn['评分'].mean())\n",
    "    Xn['点评数'] = np.log1p(Xn['点评数'].fillna(Xn['点评数'].mean()))\n",
    "\n",
    "    Xb = df[binary_features].fillna(0).astype(float)\n",
    "    Xi = np.log1p(df[index_features].copy().fillna(df[index_features].mean()))\n",
    "\n",
    "    X = pd.concat([Xn, Xb, Xi], axis=1)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    return np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "def prepare_targets(df: pd.DataFrame) -> np.ndarray:\n",
    "    cols = ['6月月销量', '7月月销量', '8月月销量', '9月月销量', '10月月销量']\n",
    "    y = df[cols].fillna(0).values\n",
    "    y = np.clip(y, a_min=0, a_max=None)\n",
    "    y = np.log1p(y)\n",
    "    return np.nan_to_num(y, nan=0.0)\n",
    "\n",
    "# ========= 清洗：NaN/常量列/标准化 =========\n",
    "\n",
    "def clean_matrix(mat: np.ndarray, name: str) -> np.ndarray:\n",
    "    if np.isnan(mat).any():\n",
    "        col_means = np.nanmean(mat, axis=0)\n",
    "        inds = np.where(np.isnan(mat))\n",
    "        mat[inds] = np.take(col_means, inds[1])\n",
    "    var = np.var(mat, axis=0)\n",
    "    keep = var > 1e-6\n",
    "    dropped = int((~keep).sum())\n",
    "    if dropped:\n",
    "        print(f\"{name}: 移除常量列 {dropped} 个\")\n",
    "    mat = mat[:, keep] if keep.any() else mat\n",
    "    mat = StandardScaler().fit_transform(mat)\n",
    "    return mat\n",
    "\n",
    "# ========= 数据集 & 模型 =========\n",
    "\n",
    "class TourismDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "class HuberMSELoss(nn.Module):\n",
    "    def __init__(self, delta=1.0, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "        self.mse   = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.mse(pred, target) + (1 - self.alpha) * self.huber(pred, target)\n",
    "\n",
    "class FeatureFusionMLP(nn.Module):\n",
    "    def __init__(self, basic_dim, layoutxlm_dim=None, visual_dim=None, nonvisual_dim=None, reduced_dim=64):\n",
    "        super().__init__()\n",
    "        self.basic_reduce     = nn.Linear(basic_dim, reduced_dim)\n",
    "        self.layoutxlm_reduce = nn.Linear(layoutxlm_dim, reduced_dim) if layoutxlm_dim else None\n",
    "        self.visual_reduce    = nn.Linear(visual_dim, reduced_dim)    if visual_dim else None\n",
    "        self.nonvisual_reduce = nn.Linear(nonvisual_dim, reduced_dim) if nonvisual_dim else None\n",
    "\n",
    "        input_dim = reduced_dim\n",
    "        if self.layoutxlm_reduce: input_dim += reduced_dim\n",
    "        if self.visual_reduce:    input_dim += reduced_dim\n",
    "        if self.nonvisual_reduce: input_dim += reduced_dim\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32), nn.ReLU(), nn.BatchNorm1d(32), nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16), nn.ReLU(), nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        start = 0\n",
    "        b_in = self.basic_reduce.in_features\n",
    "        basic = x[:, start:start+b_in]; start += b_in\n",
    "\n",
    "        lxlm = None\n",
    "        if self.layoutxlm_reduce:\n",
    "            l_in = self.layoutxlm_reduce.in_features\n",
    "            lxlm = x[:, start:start+l_in]; start += l_in\n",
    "\n",
    "        vis = None\n",
    "        if self.visual_reduce:\n",
    "            v_in = self.visual_reduce.in_features\n",
    "            vis = x[:, start:start+v_in]; start += v_in\n",
    "\n",
    "        nonvis = None\n",
    "        if self.nonvisual_reduce:\n",
    "            n_in = self.nonvisual_reduce.in_features\n",
    "            nonvis = x[:, start:start+n_in]\n",
    "\n",
    "        b = self.basic_reduce(basic)\n",
    "        l = self.layoutxlm_reduce(lxlm) if lxlm is not None else torch.zeros_like(b)\n",
    "        v = self.visual_reduce(vis)      if vis   is not None else torch.zeros_like(b)\n",
    "        n = self.nonvisual_reduce(nonvis)if nonvis is not None else torch.zeros_like(b)\n",
    "\n",
    "        feats = [b]\n",
    "        if lxlm is not None: feats.append(l)\n",
    "        if vis  is not None: feats.append(v)\n",
    "        if nonvis is not None: feats.append(n)\n",
    "        return self.net(torch.cat(feats, dim=1))\n",
    "\n",
    "# ========= 训练/验证/测试 =========\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, test_loader, device, epochs=100):\n",
    "    criterion = HuberMSELoss(delta=1.0, alpha=0.7)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optim, max_lr=5e-4, epochs=epochs, steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1, anneal_strategy='cos'\n",
    "    )\n",
    "    best_val, best_state, patience, cnt = float('inf'), None, 10, 0\n",
    "    for ep in range(epochs):\n",
    "        model.train(); tl = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optim.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb.view(-1,1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step(); sched.step()\n",
    "            tl += loss.item()\n",
    "        tl /= max(1, len(train_loader))\n",
    "\n",
    "        model.eval(); vl = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                vl += criterion(model(xb), yb.view(-1,1)).item()\n",
    "        vl /= max(1, len(val_loader))\n",
    "\n",
    "        if vl < best_val:\n",
    "            best_val, best_state, cnt = vl, model.state_dict(), 0\n",
    "        else:\n",
    "            cnt += 1\n",
    "            if cnt >= patience:\n",
    "                print(f\"Early stopping @ epoch {ep+1}\")\n",
    "                break\n",
    "        if ep == 0 or (ep + 1) % 10 == 0:\n",
    "            print(f\"Epoch {ep+1:03d} | Train {tl:.4f} | Val {vl:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Test\n",
    "    model.eval(); preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pr = model(xb)\n",
    "            preds.append(pr.cpu().numpy()); gts.append(yb.cpu().numpy())\n",
    "    y_pred = np.concatenate(preds, axis=0).reshape(-1)\n",
    "    y_true = np.concatenate(gts, axis=0).reshape(-1)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# ========= 主流程（模型A vs 模型C） =========\n",
    "\n",
    "def run_A_C(data_path, base_path=r\"\", epochs=100, seed=42):\n",
    "    np.random.seed(seed); torch.manual_seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"读取数据...\")\n",
    "    df_all = pd.read_excel(data_path)\n",
    "\n",
    "    print(\"抽取统一子集：LayoutXLM + 视觉均值向量.txt + 视觉+两种感官及以上均值向量.txt\")\n",
    "    lxlm_mat, vis_mean_mat, two_more_mat, valid_idx = extract_subset_for_A_C(df_all, base_path)\n",
    "    df = df_all.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "    print(\"清洗 & 标准化...\")\n",
    "    lxlm_mat   = clean_matrix(lxlm_mat,   \"LayoutXLM\")\n",
    "    vis_mean_mat   = clean_matrix(vis_mean_mat,   \"VisualMean\")\n",
    "    two_more_mat   = clean_matrix(two_more_mat,   \"TwoOrMore\")\n",
    "\n",
    "    print(\"准备 Baseline+百度 与 目标...\")\n",
    "    X_basic = prepare_baseline_data(df)\n",
    "    y_mat   = prepare_targets(df)    # [N, 5]\n",
    "    y_flat  = y_mat.reshape(-1, 1)   # [N*5, 1]\n",
    "\n",
    "    basic_dim     = X_basic.shape[1]\n",
    "    layoutxlm_dim = lxlm_mat.shape[1]\n",
    "    visual_dim    = vis_mean_mat.shape[1]\n",
    "    two_dim       = two_more_mat.shape[1]\n",
    "\n",
    "   \n",
    "    X_A = np.concatenate([X_basic, lxlm_mat, vis_mean_mat], axis=1)   # 模型A\n",
    "    X_C = np.concatenate([X_basic, lxlm_mat, two_more_mat], axis=1)   # 模型C\n",
    "    X_A_rep = np.repeat(X_A, 5, axis=0)\n",
    "    X_C_rep = np.repeat(X_C, 5, axis=0)\n",
    "\n",
    "   \n",
    "    Nrep = X_A_rep.shape[0]\n",
    "    all_idx = np.arange(Nrep)\n",
    "    idx_train_val, idx_test = train_test_split(all_idx, test_size=0.2, random_state=seed, shuffle=True)\n",
    "    idx_train, idx_val = train_test_split(idx_train_val, test_size=0.2, random_state=seed, shuffle=True)\n",
    "\n",
    "    def make_loaders(Xrep):\n",
    "        X_train, X_val, X_test = Xrep[idx_train], Xrep[idx_val], Xrep[idx_test]\n",
    "        y_train, y_val, y_test = y_flat[idx_train], y_flat[idx_val], y_flat[idx_test]\n",
    "        bs = min(32, max(1, len(idx_train)//8)) if len(idx_train) > 0 else 16\n",
    "        bs = max(8, min(64, bs))\n",
    "        return (\n",
    "            DataLoader(TourismDataset(X_train, y_train), batch_size=bs, shuffle=True),\n",
    "            DataLoader(TourismDataset(X_val,   y_val),   batch_size=bs),\n",
    "            DataLoader(TourismDataset(X_test,  y_test),  batch_size=bs),\n",
    "        )\n",
    "\n",
    "    trA, vaA, teA = make_loaders(X_A_rep)\n",
    "    trC, vaC, teC = make_loaders(X_C_rep)\n",
    "\n",
    "    print(\"\\n训练模型A：Baseline + 百度 + LayoutXLM + 视觉（来自 视觉均值向量.txt）\")\n",
    "    modelA = FeatureFusionMLP(basic_dim, layoutxlm_dim, visual_dim, None).to(device)\n",
    "    resA = train_eval(modelA, trA, vaA, teA, device, epochs=epochs)\n",
    "\n",
    "    print(\"\\n训练模型C：Baseline + 百度 + LayoutXLM + 两种及以上（来自 视觉+两种感官及以上均值向量.txt）\")\n",
    "    modelC = FeatureFusionMLP(basic_dim, layoutxlm_dim, None, two_dim).to(device)\n",
    "    resC = train_eval(modelC, trC, vaC, teC, device, epochs=epochs)\n",
    "\n",
    "    print(\"\\n模型性能对比：\")\n",
    "    print(f\"{'模型':<55} {'MSE':>12} {'RMSE':>12} {'R2':>10}\")\n",
    "    print(f\"{'A) Baseline+Index+LayoutXLM+Visual(mean)':<55} {resA['MSE']:12.4f} {resA['RMSE']:12.4f} {resA['R2']:10.4f}\")\n",
    "    print(f\"{'C) Baseline+Index+LayoutXLM+Two-or-More NV':<55} {resC['MSE']:12.4f} {resC['RMSE']:12.4f} {resC['R2']:10.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = r\"\"\n",
    "    run_A_C(data_path, base_path=r\"\", epochs=100, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0f065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3x3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "-\n",
    "\n",
    "def read_feature_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read().strip().strip('[]')\n",
    "            if not content:\n",
    "                return None\n",
    "            features = [float(x.strip()) for x in content.split(',')]\n",
    "            return np.array(features, dtype=float)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_layoutxlm_features(product_id, province, base_path):\n",
    "    product_path = os.path.join(base_path, str(province), str(product_id), '')\n",
    "    if not os.path.exists(product_path):\n",
    "        return None\n",
    "    cls_path = os.path.join(product_path, '')\n",
    "    img_path = os.path.join(product_path, '')\n",
    "    cls_features = read_feature_file(cls_path)\n",
    "    img_features = read_feature_file(img_path)\n",
    "    if cls_features is None or img_features is None:\n",
    "        return None\n",
    "    return np.concatenate([cls_features, img_features])\n",
    "\n",
    "def get_visual_features(product_id, province, base_path):\n",
    "    visual_path = os.path.join(base_path, str(province), str(product_id), '', '')\n",
    "    if not os.path.exists(visual_path):\n",
    "        return None\n",
    "    visual_features = read_feature_file(visual_path)\n",
    "    if visual_features is None:\n",
    "        return None\n",
    "    # 视觉特征按 768 维对齐\n",
    "    if len(visual_features) < 768:\n",
    "        visual_features = np.pad(visual_features, (0, 768 - len(visual_features)), 'constant')\n",
    "    elif len(visual_features) > 768:\n",
    "        visual_features = visual_features[:768]\n",
    "    return visual_features\n",
    "\n",
    "def extract_all_features(df, base_path=r\"\"):\n",
    "    \"\"\"\n",
    "    仅保留 layoutxlm 与 visual 同时存在的样本；\n",
    "    - 填充 NaN（列均值）\n",
    "    - 去除常量列\n",
    "    - 标准化（各自 fit_transform）\n",
    "    \"\"\"\n",
    "    layoutxlm_features, visual_features, valid_indices = [], [], []\n",
    "    total = len(df)\n",
    "    found = 0\n",
    "    print(f\"[特征抽取] 处理 {total} 条样本...\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        pid = row['ID']\n",
    "        prov = row['省份']\n",
    "        lxlm = get_layoutxlm_features(pid, prov, base_path)\n",
    "        vis  = get_visual_features(pid, prov, base_path)\n",
    "        if lxlm is not None and vis is not None:\n",
    "            layoutxlm_features.append(lxlm)\n",
    "            visual_features.append(vis)\n",
    "            valid_indices.append(i)\n",
    "            found += 1\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  已处理 {i + 1}/{total}，有效样本 {found}\")\n",
    "\n",
    "    if not layoutxlm_features or not visual_features:\n",
    "        raise ValueError(\"未找到任何有效的（同时具备 layoutxlm 与 视觉）样本。\")\n",
    "\n",
    "    lxlm_mat = np.stack(layoutxlm_features).astype(float)\n",
    "    vis_mat  = np.stack(visual_features).astype(float)\n",
    "\n",
    "    # 列均值填充 NaN\n",
    "    for arr in (lxlm_mat, vis_mat):\n",
    "        nan_mask = np.isnan(arr)\n",
    "        if nan_mask.any():\n",
    "            col_means = np.nanmean(arr, axis=0)\n",
    "            inds = np.where(nan_mask)\n",
    "            arr[inds] = np.take(col_means, inds[1])\n",
    "\n",
    "    # 去常量列\n",
    "    def remove_constant_cols(mat, name):\n",
    "        var = np.var(mat, axis=0)\n",
    "        keep = var > 1e-6\n",
    "        removed = int((~keep).sum())\n",
    "        if removed > 0:\n",
    "            print(f\"[特征抽取] {name} 去除常量列 {removed} 个\")\n",
    "        return mat[:, keep]\n",
    "\n",
    "    lxlm_mat = remove_constant_cols(lxlm_mat, \"LayoutXLM\")\n",
    "    vis_mat  = remove_constant_cols(vis_mat,  \"Visual\")\n",
    "\n",
    "    # 标准化\n",
    "    lxlm_scaler = StandardScaler()\n",
    "    vis_scaler  = StandardScaler()\n",
    "    lxlm_mat = lxlm_scaler.fit_transform(lxlm_mat)\n",
    "    vis_mat  = vis_scaler.fit_transform(vis_mat)\n",
    "\n",
    "    print(f\"[特征抽取] 完成。有效样本: {len(valid_indices)}\")\n",
    "    return lxlm_mat, vis_mat, valid_indices\n",
    "\n",
    "\n",
    "def prepare_baseline_only(df_sub):\n",
    "    \"\"\"三项数值 + 五项二元，共 8 维；标准化\"\"\"\n",
    "    numeric = ['价格', '评分', '点评数']\n",
    "    binary  = ['无购物', '无自费', '成团保障', '退改政策', '是否促销']\n",
    "\n",
    "    Xn = df_sub[numeric].copy()\n",
    "    Xn['价格']  = pd.to_numeric(Xn['价格'], errors='coerce')\n",
    "    Xn['评分']  = pd.to_numeric(Xn['评分'], errors='coerce')\n",
    "    Xn['点评数'] = pd.to_numeric(Xn['点评数'], errors='coerce')\n",
    "\n",
    "    Xn['价格']  = Xn['价格'].fillna(Xn['价格'].mean())\n",
    "    Xn['评分']  = Xn['评分'].fillna(Xn['评分'].mean())\n",
    "    Xn['点评数'] = Xn['点评数'].fillna(Xn['点评数'].mean())\n",
    "\n",
    "    Xn['价格']  = np.log1p(Xn['价格'])\n",
    "    Xn['点评数'] = np.log1p(Xn['点评数'])\n",
    "\n",
    "    Xb = df_sub[binary].fillna(0).astype(float)\n",
    "    X = pd.concat([Xn, Xb], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return np.nan_to_num(X_scaled, nan=0.0)\n",
    "\n",
    "def prepare_baseline_plus_baidu(df_sub):\n",
    "    \"\"\"三项数值 + 五项二元 + 五项百度指数，共 13 维；标准化\"\"\"\n",
    "    numeric = ['价格', '评分', '点评数']\n",
    "    binary  = ['无购物', '无自费', '成团保障', '退改政策', '是否促销']\n",
    "    index5  = ['index_6', 'index_7', 'index_8', 'index_9', 'index_10']\n",
    "\n",
    "    Xn = df_sub[numeric].copy()\n",
    "    Xn['价格']  = pd.to_numeric(Xn['价格'], errors='coerce')\n",
    "    Xn['评分']  = pd.to_numeric(Xn['评分'], errors='coerce')\n",
    "    Xn['点评数'] = pd.to_numeric(Xn['点评数'], errors='coerce')\n",
    "\n",
    "    Xn['价格']  = Xn['价格'].fillna(Xn['价格'].mean())\n",
    "    Xn['评分']  = Xn['评分'].fillna(Xn['评分'].mean())\n",
    "    Xn['点评数'] = Xn['点评数'].fillna(Xn['点评数'].mean())\n",
    "\n",
    "    Xn['价格']  = np.log1p(Xn['价格'])\n",
    "    Xn['点评数'] = np.log1p(Xn['点评数'])\n",
    "\n",
    "    Xb = df_sub[binary].fillna(0).astype(float)\n",
    "\n",
    "    Xi = df_sub[index5].copy()\n",
    "    for c in index5:\n",
    "        Xi[c] = pd.to_numeric(Xi[c], errors='coerce')\n",
    "    Xi = Xi.fillna(Xi.mean())\n",
    "    Xi = np.log1p(Xi)\n",
    "\n",
    "    X = pd.concat([Xn, Xb, Xi], axis=1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return np.nan_to_num(X_scaled, nan=0.0)\n",
    "\n",
    "def prepare_targets(df_sub):\n",
    "    \"\"\"目标：6-10 月销量；clip>=0；log1p；返回 [n, 5]\"\"\"\n",
    "    cols = ['6月月销量', '7月月销量', '8月月销量', '9月月销量', '10月月销量']\n",
    "    y = df_sub[cols].copy()\n",
    "    for c in cols:\n",
    "        y[c] = pd.to_numeric(y[c], errors='coerce').fillna(0.0)\n",
    "        y[c] = np.clip(y[c].values, a_min=0.0, a_max=None)\n",
    "        y[c] = np.log1p(y[c].values)\n",
    "    return y.values.astype(float)\n",
    "\n",
    "\n",
    "\n",
    "class TourismDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class HuberMSELoss(nn.Module):\n",
    "    def __init__(self, delta=1.0, alpha=0.7):\n",
    "        super().__init__()\n",
    "        self.huber = nn.HuberLoss(delta=delta)\n",
    "        self.mse   = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        return self.alpha * self.mse(pred, target) + (1 - self.alpha) * self.huber(pred, target)\n",
    "\n",
    "class FeatureFusionMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, basic_dim, layoutxlm_dim=None, visual_dim=None, reduced_dim=64):\n",
    "        super().__init__()\n",
    "        self.basic_dim = int(basic_dim)\n",
    "        self.layoutxlm_dim = int(layoutxlm_dim) if layoutxlm_dim is not None and layoutxlm_dim > 0 else None\n",
    "        self.visual_dim    = int(visual_dim)    if visual_dim is not None and visual_dim > 0    else None\n",
    "\n",
    "        self.basic_reduce = nn.Linear(self.basic_dim, reduced_dim)\n",
    "        self.layoutxlm_reduce = nn.Linear(self.layoutxlm_dim, reduced_dim) if self.layoutxlm_dim else None\n",
    "        self.visual_reduce    = nn.Linear(self.visual_dim, reduced_dim)    if self.visual_dim    else None\n",
    "\n",
    "        branches = 1\n",
    "        if self.layoutxlm_reduce: branches += 1\n",
    "        if self.visual_reduce:    branches += 1\n",
    "        input_dim = reduced_dim * branches\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        start = 0\n",
    "        basic = x[:, start:start + self.basic_dim]; start += self.basic_dim\n",
    "\n",
    "        lxlm = None\n",
    "        if self.layoutxlm_dim:\n",
    "            lxlm = x[:, start:start + self.layoutxlm_dim]; start += self.layoutxlm_dim\n",
    "\n",
    "        visual = None\n",
    "        if self.visual_dim:\n",
    "            visual = x[:, start:start + self.visual_dim]; start += self.visual_dim\n",
    "\n",
    "        out_list = [self.basic_reduce(basic)]\n",
    "        if self.layoutxlm_reduce is not None and lxlm is not None:\n",
    "            out_list.append(self.layoutxlm_reduce(lxlm))\n",
    "        if self.visual_reduce is not None and visual is not None:\n",
    "            out_list.append(self.visual_reduce(visual))\n",
    "\n",
    "        fused = torch.cat(out_list, dim=1)\n",
    "        return self.net(fused)\n",
    "\n",
    "\n",
    "\n",
    "def train_eval(model, train_loader, val_loader, test_loader, device, epochs=100):\n",
    "    criterion = HuberMSELoss(delta=1.0, alpha=0.7)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=5e-4, epochs=epochs, steps_per_epoch=max(1, len(train_loader)), pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    patience, counter = 10, 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).view(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device).view(-1, 1)\n",
    "                pred = model(xb)\n",
    "                va_loss += criterion(pred, yb).item()\n",
    "\n",
    "        tr_avg = tr_loss / max(1, len(train_loader))\n",
    "        va_avg = va_loss / max(1, len(val_loader))\n",
    "\n",
    "        if va_avg < best_val:\n",
    "            best_val = va_avg\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f\"  [早停] epoch {ep+1}\")\n",
    "            break\n",
    "\n",
    "        if (ep + 1) % 10 == 0 or ep == 0:\n",
    "            print(f\"  Epoch {ep+1:3d} | Train {tr_avg:.4f} | Val {va_avg:.4f}\")\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # 测试\n",
    "    model.eval()\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb).cpu().numpy().reshape(-1)\n",
    "            preds.append(pred)\n",
    "            gts.append(yb.numpy().reshape(-1))\n",
    "    yhat = np.concatenate(preds)\n",
    "    y    = np.concatenate(gts)\n",
    "\n",
    "    mse  = mean_squared_error(y, yhat)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2   = r2_score(y, yhat)\n",
    "    return {'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "\n",
    "\n",
    "def _build_model(tag, dims, device):\n",
    "    if tag == 'Baseline':\n",
    "        return FeatureFusionMLP(dims['basic_dim_x0'], None, None).to(device)\n",
    "    elif tag == 'Baseline+百度':\n",
    "        return FeatureFusionMLP(dims['basic_dim_x1'], None, None).to(device)\n",
    "    elif tag == 'Baseline+百度+LayoutXLM':\n",
    "        return FeatureFusionMLP(dims['basic_dim_x1'], dims['layoutxlm_dim'], None).to(device)\n",
    "    elif tag == 'Baseline+百度+LayoutXLM+视觉':\n",
    "        return FeatureFusionMLP(dims['basic_dim_x1'], dims['layoutxlm_dim'], dims['visual_dim']).to(device)\n",
    "    elif tag == 'Baseline+百度+视觉':\n",
    "        return FeatureFusionMLP(dims['basic_dim_x1'], None, dims['visual_dim']).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"未知模型: {tag}\")\n",
    "\n",
    "def _eval_subset(name, mask, X0, X1, X2, X3, X4, y5, dims, device):\n",
    "    idx = np.where(mask)[0]\n",
    "    n = len(idx)\n",
    "    if n < 8:  \n",
    "        print(f\"[跳过] {name} 样本不足（n={n}）\")\n",
    "        return None\n",
    "\n",
    "    X0_s, X1_s, X2_s, X3_s, X4_s = X0[idx], X1[idx], X2[idx], X3[idx], X4[idx]\n",
    "    y_s = y5[idx]  # [n,5]\n",
    "\n",
    "    def rep5(a): return np.repeat(a, 5, axis=0)\n",
    "    X_dict = {\n",
    "        'Baseline':                        rep5(X0_s),\n",
    "        'Baseline+百度':                   rep5(X1_s),\n",
    "        'Baseline+百度+LayoutXLM':         rep5(X2_s),\n",
    "        'Baseline+百度+LayoutXLM+视觉':     rep5(X3_s),\n",
    "        'Baseline+百度+视觉':              rep5(X4_s),\n",
    "    }\n",
    "    y_rep = y_s.reshape(-1)\n",
    "\n",
    "    results = {}\n",
    "    for tag, X in X_dict.items():\n",
    "        try:\n",
    "            X_trv, X_te, y_trv, y_te = train_test_split(X, y_rep, test_size=0.2, random_state=42)\n",
    "            if len(X_trv) == 0 or len(X_te) == 0:\n",
    "                print(f\"[跳过] {name} - {tag} 切分不足\")\n",
    "                continue\n",
    "            X_tr, X_va, y_tr, y_va = train_test_split(X_trv, y_trv, test_size=0.2, random_state=42)\n",
    "            if len(X_tr) == 0 or len(X_va) == 0:\n",
    "                print(f\"[跳过] {name} - {tag} 训练/验证不足\")\n",
    "                continue\n",
    "\n",
    "            bs = max(1, min(32, len(X_tr)))\n",
    "            tr_loader = DataLoader(TourismDataset(X_tr, y_tr), batch_size=bs, shuffle=True)\n",
    "            va_loader = DataLoader(TourismDataset(X_va, y_va), batch_size=bs)\n",
    "            te_loader = DataLoader(TourismDataset(X_te, y_te), batch_size=bs)\n",
    "\n",
    "            model = _build_model(tag, dims, device)\n",
    "            res = train_eval(model, tr_loader, va_loader, te_loader, device)\n",
    "            results[tag] = res\n",
    "        except Exception as e:\n",
    "            print(f\"[跳过] {name} - {tag} 发生异常：{e}\")\n",
    "\n",
    "    # 打印\n",
    "    if results:\n",
    "        print(f\"\\n========== {name}（n={n}） ==========\")\n",
    "        print(f\"{'模型名称':<30} {'MSE':>12} {'RMSE':>12} {'R2':>10}\")\n",
    "        for tag, m in results.items():\n",
    "            print(f\"{tag:<30} {m['MSE']:12.4f} {m['RMSE']:12.4f} {m['R2']:10.4f}\")\n",
    "    else:\n",
    "        print(f\"[提示] {name} 无可用结果。\")\n",
    "    return results if results else None\n",
    "\n",
    "def run_compare_by_price_and_top3types_cross(data_path, base_path=r\"D:\\有效数据汇总\"):\n",
    "    print(\"[读取] Excel 数据...\")\n",
    "    df = pd.read_excel(data_path)\n",
    "\n",
    "   \n",
    "    lxlm_mat, vis_mat, valid_idx = extract_all_features(df, base_path)\n",
    "    dfc = df.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "   \n",
    "    print(\"[准备] Baseline / Baseline+百度 / 目标...\")\n",
    "    X0 = prepare_baseline_only(dfc)\n",
    "    X1 = prepare_baseline_plus_baidu(dfc)\n",
    "    X2 = np.concatenate([X1, lxlm_mat], axis=1)\n",
    "    X3 = np.concatenate([X1, lxlm_mat, vis_mat], axis=1)\n",
    "    X4 = np.concatenate([X1, vis_mat], axis=1)\n",
    "    y5 = prepare_targets(dfc)  # [n,5]\n",
    "\n",
    "    dims = {\n",
    "        'basic_dim_x0': X0.shape[1],\n",
    "        'basic_dim_x1': X1.shape[1],\n",
    "        'layoutxlm_dim': lxlm_mat.shape[1],\n",
    "        'visual_dim': vis_mat.shape[1],\n",
    "    }\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  \n",
    "    price = pd.to_numeric(dfc['价格'], errors='coerce')\n",
    "    masks_price = {\n",
    "        '低价(<1340)':        (price < 1340).values,\n",
    "        '中价(1340-3189)':    ((price >= 1340) & (price <= 3189)).values,\n",
    "        '高价(>3189)':        (price > 3189).values,\n",
    "    }\n",
    "\n",
    "  \n",
    "    type_series = dfc['类型'].astype(str).fillna('')\n",
    "    skip_types = {'游学', '半自由行', '半自助游'}\n",
    "    type_series_filtered = type_series[~type_series.isin(skip_types)]\n",
    "    vc = type_series_filtered.value_counts()\n",
    "    selected_types = list(vc.index[:3])\n",
    "\n",
    "    if len(selected_types) < 3:\n",
    "        print(f\"[提示] 可用产品类型不足 3 个，实际：{selected_types}\")\n",
    "\n",
    "    print(f\"\\n[信息] 选取的产品类型 Top3：{selected_types}\")\n",
    "\n",
    "  \n",
    "    results_cross = {}\n",
    "    for t in selected_types:\n",
    "        t_mask = (type_series == t).values\n",
    "        for price_name, p_mask in masks_price.items():\n",
    "            name = f\"类型：{t} × 价格：{price_name}\"\n",
    "            mask = t_mask & p_mask\n",
    "            r = _eval_subset(name, mask, X0, X1, X2, X3, X4, y5, dims, device)\n",
    "            if r is not None:\n",
    "                results_cross[(t, price_name)] = r\n",
    "\n",
    "\n",
    "    return results_cross\n",
    "\n",
    "# ------------------------ 入口 ------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    DATA_PATH = r\"\"\n",
    "    BASE_PATH = r\"\"\n",
    "    run_compare_by_price_and_top3types_cross(DATA_PATH, BASE_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYPAPPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
