{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865bfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"\"),  # \n",
    "    base_url=\"\"\n",
    ")\n",
    "\n",
    "sensory_map = {\n",
    "}\n",
    "\n",
    "total_tokens = 0\n",
    "root_dir = r\"\"\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "for province in tqdm(os.listdir(root_dir), desc=\"éå†çœä»½\"):\n",
    "    province_path = os.path.join(root_dir, province)\n",
    "    if not os.path.isdir(province_path):\n",
    "        continue\n",
    "\n",
    "    for project_id in os.listdir(province_path):\n",
    "        project_path = os.path.join(province_path, project_id)\n",
    "        if not os.path.isdir(project_path):\n",
    "            continue\n",
    "\n",
    "        result_dir = os.path.join(project_path, \"\")\n",
    "        concat_file = os.path.join(result_dir, \"\")\n",
    "\n",
    "        if not os.path.isfile(concat_file):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = pd.read_excel(concat_file)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ— æ³•è¯»å–æ–‡ä»¶ï¼š{concat_file}ï¼Œè·³è¿‡ã€‚åŸå› ï¼š{e}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nğŸ“ æ­£åœ¨å¤„ç†ï¼š{province}/{project_id}\")\n",
    "\n",
    "        for col, filename in sensory_map.items():\n",
    "            if col not in df.columns:\n",
    "                print(f\"âš ï¸ åˆ— {col} ä¸å­˜åœ¨ï¼Œè·³è¿‡\")\n",
    "                continue\n",
    "\n",
    "            words_series = df[col].dropna().astype(str).str.split('ã€')\n",
    "            word_list = sorted(set(\n",
    "                word.strip() for sublist in words_series for word in sublist if word.strip()\n",
    "            ))\n",
    "\n",
    "            if not word_list:\n",
    "                print(f\"âš ï¸  è·³è¿‡ç©ºåˆ—ï¼š{col}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"â¡ï¸  æ­£åœ¨æå– {col}ï¼š{len(word_list)} ä¸ªè¯\")\n",
    "            print(f\"ç¤ºä¾‹è¯æ±‡ï¼š{word_list[:10]}\")\n",
    "\n",
    "            embeddings = []\n",
    "            batch_size = 10\n",
    "\n",
    "            try:\n",
    "                for batch in chunks(word_list, batch_size):\n",
    "                    response = client.embeddings.create(\n",
    "                        model=\"\",\n",
    "                        input=batch,\n",
    "                        dimensions=768,          \n",
    "                        encoding_format=\"float\"  \n",
    "                    )\n",
    "                    print(f\"  æœ¬æ‰¹è¯·æ±‚è¯æ•°ï¼š{len(batch)}ï¼Œè¿”å›å‘é‡æ•°ï¼š{len(response.data)}\")\n",
    "                    embeddings.extend(response.data)\n",
    "                    total_tokens += getattr(response.usage, 'total_tokens', 0)\n",
    "\n",
    "                print(f\"   âœ… å·²å®Œæˆ {col}ï¼Œç´¯è®¡tokenï¼š{total_tokens}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ åµŒå…¥å¤±è´¥ï¼š{col}ï¼ŒåŸå› ï¼š{e}\")\n",
    "                continue\n",
    "\n",
    "       \n",
    "            vectors_np = np.array([item.embedding for item in embeddings])\n",
    "            mean_vector = np.mean(vectors_np, axis=0)\n",
    "\n",
    "         \n",
    "            output_path = os.path.join(result_dir, filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                vector_str = ','.join(map(str, mean_vector))\n",
    "                f.write(vector_str)\n",
    "\n",
    "            print(f\"   ğŸ“„ å·²ä¿å­˜ {col} ç»´åº¦å‡å€¼å‘é‡æ–‡ä»¶ï¼š{output_path}\")\n",
    "\n",
    "print(\"\\n=== âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæ¯• ===\")\n",
    "print(f\"ğŸ”¢ æ€» Token æ¶ˆè€—ï¼š{total_tokens}\")\n",
    "cost = total_tokens / 1000 * 0.0005\n",
    "print(f\"ğŸ’° æ€»è´¹ç”¨ä¼°ç®—ï¼šï¿¥{cost:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MYPAPPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
